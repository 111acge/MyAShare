{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T14:39:10.779695Z",
     "start_time": "2025-02-05T14:39:10.776286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#####################\n",
    "### 导入部分 ###\n",
    "#####################\n",
    "import akshare as ak\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime, timedelta"
   ],
   "id": "7917a789c0a5f179",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T14:40:12.443212Z",
     "start_time": "2025-02-05T14:39:12.056874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#####################\n",
    "###  公司基本信息  ###\n",
    "#####################\n",
    "def get_company_info(code):\n",
    "    # 获取市场前缀\n",
    "    symbol = f\"{code}\"\n",
    "    info_dict = {}\n",
    "\n",
    "    try:\n",
    "        # 基础信息\n",
    "        base_info = ak.stock_individual_info_em(symbol=symbol)\n",
    "        info_dict['股票简称'] = base_info.loc[base_info['item'] == '股票简称', 'value'].values[0]\n",
    "        info_dict['行业'] = base_info.loc[base_info['item'] == '行业', 'value'].values[0]\n",
    "        info_dict['上市时间'] = base_info.loc[base_info['item'] == '上市时间', 'value'].values[0]\n",
    "    except Exception as e:\n",
    "        print(f\"基础信息获取失败: {str(e)}\")\n",
    "        info_dict.update({'股票简称': '未知', '行业': '未知', '上市时间': '未知'})\n",
    "\n",
    "    try:\n",
    "        # 发行信息\n",
    "        stock_ipo_info_df = ak.stock_ipo_info(stock=symbol)\n",
    "        if not stock_ipo_info_df.empty:\n",
    "            info_dict['发行价'] = stock_ipo_info_df.loc[stock_ipo_info_df['item'] == '发行价(元)', 'value'].values[0]\n",
    "        else:\n",
    "            info_dict['发行价'] = '暂无数据'\n",
    "    except Exception as e:\n",
    "        print(f\"发行价获取失败: {str(e)}\")\n",
    "        info_dict['发行价'] = '暂无数据'\n",
    "\n",
    "    try:\n",
    "        # 分红信息\n",
    "        stock_history_dividend_df = ak.stock_history_dividend()\n",
    "        dividend_info = stock_history_dividend_df[stock_history_dividend_df['代码'] == code]\n",
    "        # 如果找到记录，获取分红次数列的值；如果没找到记录，则为0\n",
    "        info_dict['分红次数'] = dividend_info['分红次数'].iloc[0] if not dividend_info.empty else 0\n",
    "    except Exception as e:\n",
    "        print(f\"分红信息获取失败: {str(e)}\")\n",
    "        info_dict['分红次数'] = 0\n",
    "\n",
    "    try:\n",
    "        # 机构参与度\n",
    "        jg_info = ak.stock_comment_detail_zlkp_jgcyd_em(symbol=symbol)\n",
    "        info_dict['机构参与度'] = f\"{jg_info['机构参与度'].values[0]}%\"\n",
    "    except Exception as e:\n",
    "        print(f\"机构参与度获取失败: {str(e)}\")\n",
    "        info_dict['机构参与度'] = '暂无数据'\n",
    "\n",
    "    try:\n",
    "        # 市场成本\n",
    "        cost_info = ak.stock_comment_detail_scrd_cost_em(symbol=symbol)\n",
    "        info_dict['市场成本'] = f\"{cost_info['市场成本'].values[0]}元\"\n",
    "    except Exception as e:\n",
    "        print(f\"市场成本获取失败: {str(e)}\")\n",
    "        info_dict['市场成本'] = '暂无数据'\n",
    "\n",
    "    # 格式化输出\n",
    "    print(f\"\\n===== {code} 公司基本信息 =====\")\n",
    "    print(f\"股票简称：{info_dict['股票简称']}\")\n",
    "    print(f\"所属行业：{info_dict['行业']}\")\n",
    "    print(f\"上市时间：{info_dict['上市时间']}\")\n",
    "    print(f\"发行价格：{info_dict['发行价']}\")\n",
    "    print(f\"分红次数：{info_dict['分红次数']}次\")\n",
    "    print(f\"机构参与：{info_dict['机构参与度']}\")\n",
    "    print(f\"成本均价：{info_dict['市场成本']}\")\n",
    "\n",
    "    return info_dict\n",
    "\n",
    "\n",
    "stock_code = input(\"请输入6位股票代码: \")\n",
    "company_info = get_company_info(stock_code)"
   ],
   "id": "34d9722a674bcce3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "市场成本获取失败: 'NoneType' object is not subscriptable\n",
      "\n",
      "===== 600406 公司基本信息 =====\n",
      "股票简称：国电南瑞\n",
      "所属行业：电网设备\n",
      "上市时间：20031016\n",
      "发行价格：10.39\n",
      "分红次数：21次\n",
      "机构参与：27.6596%\n",
      "成本均价：暂无数据\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T14:40:48.079424Z",
     "start_time": "2025-02-05T14:40:45.054052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#####################\n",
    "###  获取历史数据   ###\n",
    "#####################\n",
    "def get_history_data(code):\n",
    "    symbol = f\"{code}\"\n",
    "    days = 365\n",
    "    end_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "    start_date = (datetime.now() - timedelta(days)).strftime(\"%Y%m%d\")\n",
    "    df = ak.stock_zh_a_hist(symbol=symbol, period=\"daily\", start_date=start_date, end_date=end_date, adjust=\"qfq\")\n",
    "    print(f\"历史{days}天数据获取完成，共获取{len(df)}条记录\")\n",
    "    return df\n",
    "\n",
    "\n",
    "history_df = get_history_data(stock_code)"
   ],
   "id": "ac4ad8a7fa934a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "历史365天数据获取完成，共获取236条记录\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T14:52:19.524642Z",
     "start_time": "2025-02-05T14:52:19.076764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#####################\n",
    "###   获取筹码分布  ###\n",
    "#####################\n",
    "def get_chip_distribution(code):\n",
    "    symbol = f\"{code}\"\n",
    "    df = ak.stock_cyq_em(symbol=symbol, adjust=\"qfq\")\n",
    "    latest_chip = df.iloc[-1].to_dict()\n",
    "    print(\n",
    "        f\"最新交易日筹码分布：获利比例={latest_chip['获利比例'] * 100:.2f}% 70集中度={(latest_chip['70集中度'] * 100):.2f}%\")\n",
    "    return latest_chip\n",
    "\n",
    "\n",
    "get_chip_distribution(stock_code)"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最新交易日筹码分布：获利比例=10.86% 70集中度=7.04%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'日期': datetime.date(2025, 2, 5),\n",
       " '获利比例': 0.10855699044741171,\n",
       " '平均成本': 24.56,\n",
       " '90成本-低': 22.37,\n",
       " '90成本-高': 27.35,\n",
       " '90集中度': 0.10012878560841076,\n",
       " '70成本-低': 23.08,\n",
       " '70成本-高': 26.58,\n",
       " '70集中度': 0.07041248413600525}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:05:32.564367Z",
     "start_time": "2025-02-05T15:05:32.550622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#####################\n",
    "###  BBIBOLL计算   ###\n",
    "#####################\n",
    "def calculate_bbiboll(df):\n",
    "    df['MA3'] = df['收盘'].rolling(3).mean()\n",
    "    df['MA6'] = df['收盘'].rolling(6).mean()\n",
    "    df['MA12'] = df['收盘'].rolling(12).mean()\n",
    "    df['MA24'] = df['收盘'].rolling(24).mean()\n",
    "\n",
    "    df['BBIBOLL'] = (df['MA3'] + df['MA6'] + df['MA12'] + df['MA24']) / 4\n",
    "    df['UPPER'] = df['BBIBOLL'] + 2 * df['BBIBOLL'].rolling(11).std()\n",
    "    df['LOWER'] = df['BBIBOLL'] - 2 * df['BBIBOLL'].rolling(11).std()\n",
    "\n",
    "    latest = df.iloc[-1][['BBIBOLL', 'UPPER', 'LOWER']].to_dict()\n",
    "    print(f\"最新BBIBOLL值: mid={latest['BBIBOLL']:.2f} upper={latest['UPPER']:.2f} lower={latest['LOWER']:.2f}\")\n",
    "    return latest\n",
    "\n",
    "\n",
    "calculate_bbiboll(history_df)"
   ],
   "id": "5b7dc2df4b122623",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最新BBIBOLL值: mid=23.30 upper=23.67 lower=22.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BBIBOLL': 23.296875,\n",
       " 'UPPER': 23.671287541751248,\n",
       " 'LOWER': 22.922462458248752}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:09:00.035588Z",
     "start_time": "2025-02-05T15:06:40.544634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "class EnhancedLSTM(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 双向LSTM层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # 注意力机制\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        # 输出层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
    "\n",
    "        # LSTM输出\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # 注意力权重\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        # 输出层\n",
    "        out = self.fc(context)\n",
    "        return out\n",
    "\n",
    "def enhanced_predict(df, target_col='收盘'):\n",
    "    \"\"\"综合多特征、参数搜索和高级LSTM结构的增强预测函数\"\"\"\n",
    "    # 准备多特征数据\n",
    "    feature_cols = ['开盘', '最高', '最低', '收盘', '成交量']\n",
    "    target_idx = feature_cols.index(target_col)\n",
    "\n",
    "    # 数据标准化\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "    # 参数网格配置\n",
    "    param_grid = {\n",
    "        'hidden_size': [64, 128],\n",
    "        'num_layers': [2, 3],\n",
    "        'dropout': [0.2, 0.3],\n",
    "        'look_back': [30, 60],\n",
    "        'learning_rate': [0.001, 0.0005],\n",
    "        'epochs': [150],\n",
    "        'patience': [15]\n",
    "    }\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "    best_params = {}\n",
    "    best_model_state = None\n",
    "    best_scaler = None\n",
    "    best_look_back = 0\n",
    "    best_predictions = None\n",
    "    best_actuals = None\n",
    "\n",
    "    # 参数搜索\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        look_back = params['look_back']\n",
    "\n",
    "        # 创建序列数据集\n",
    "        X, y = [], []\n",
    "        for i in range(len(scaled_data) - look_back):\n",
    "            X.append(scaled_data[i:i+look_back])\n",
    "            y.append(scaled_data[i+look_back, target_idx])\n",
    "\n",
    "        if len(X) < 10:  # 跳过数据量不足的参数组合\n",
    "            continue\n",
    "\n",
    "        X = torch.FloatTensor(X)\n",
    "        y = torch.FloatTensor(y).view(-1, 1)\n",
    "\n",
    "        # 模型初始化\n",
    "        model = EnhancedLSTM(\n",
    "            input_size=len(feature_cols),\n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers'],\n",
    "            dropout=params['dropout']\n",
    "        )\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # 训练循环\n",
    "        best_train_loss = float('inf')\n",
    "        stop_counter = 0\n",
    "\n",
    "        for epoch in range(params['epochs']):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # 早停机制\n",
    "            if loss.item() < best_train_loss:\n",
    "                best_train_loss = loss.item()\n",
    "                stop_counter = 0\n",
    "            else:\n",
    "                stop_counter += 1\n",
    "                if stop_counter >= params['patience']:\n",
    "                    break\n",
    "\n",
    "        # 评估模型\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(X)\n",
    "            n_samples = len(preds)\n",
    "\n",
    "            # 反标准化处理\n",
    "            dummy_preds = np.zeros((n_samples, len(feature_cols)))\n",
    "            dummy_preds[:, target_idx] = preds.numpy().flatten()\n",
    "            preds_denorm = scaler.inverse_transform(dummy_preds)[:, target_idx]\n",
    "\n",
    "            dummy_y = np.zeros((n_samples, len(feature_cols)))\n",
    "            dummy_y[:, target_idx] = y.numpy().flatten()\n",
    "            y_denorm = scaler.inverse_transform(dummy_y)[:, target_idx]\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y_denorm, preds_denorm))\n",
    "\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_params = params\n",
    "                best_model_state = model.state_dict()\n",
    "                best_scaler = scaler\n",
    "                best_look_back = look_back\n",
    "                best_predictions = preds_denorm\n",
    "                best_actuals = y_denorm\n",
    "\n",
    "    # 最终预测\n",
    "    final_model = EnhancedLSTM(\n",
    "        input_size=len(feature_cols),\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        num_layers=best_params['num_layers'],\n",
    "        dropout=best_params['dropout']\n",
    "    )\n",
    "    final_model.load_state_dict(best_model_state)\n",
    "    final_model.eval()\n",
    "\n",
    "    last_sequence = scaled_data[-best_look_back:]\n",
    "    input_tensor = torch.FloatTensor(last_sequence).view(1, best_look_back, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_pred_scaled = final_model(input_tensor).item()\n",
    "        dummy_next = np.zeros((1, len(feature_cols)))\n",
    "        dummy_next[0, target_idx] = next_pred_scaled\n",
    "        next_pred = best_scaler.inverse_transform(dummy_next)[0, target_idx]\n",
    "\n",
    "    # 输出结果\n",
    "    print(f\"\\n{target_col}价预测最佳参数: {best_params}\")\n",
    "    print(f\"训练集RMSE: {best_rmse:.2f}\")\n",
    "    print(f\"实际值={best_actuals[-1]:.2f} 预测值={best_predictions[-1]:.2f} (最新数据点)\")\n",
    "    print(f\"预测下一个交易日的{target_col}价可能为：{next_pred:.2f} 元\")\n",
    "\n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'rmse': best_rmse,\n",
    "        'last_actual': best_actuals[-1],\n",
    "        'last_pred': best_predictions[-1],\n",
    "        'next_pred': next_pred\n",
    "    }\n",
    "\n",
    "# 执行预测\n",
    "print(\"\\n增强版多特征LSTM预测结果：\")\n",
    "for col in ['开盘', '收盘', '最低', '最高']:\n",
    "    enhanced_predict(history_df, col)"
   ],
   "id": "58874e9a8147b068",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "增强版多特征LSTM预测结果：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\29193\\AppData\\Local\\Temp\\ipykernel_26116\\2192162132.py:96: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  X = torch.FloatTensor(X)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 193\u001B[0m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m增强版多特征LSTM预测结果：\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m开盘\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m收盘\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m最低\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m最高\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m--> 193\u001B[0m     \u001B[43menhanced_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhistory_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[11], line 116\u001B[0m, in \u001B[0;36menhanced_predict\u001B[1;34m(df, target_col)\u001B[0m\n\u001B[0;32m    114\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m    115\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 116\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    117\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, y)\n\u001B[0;32m    118\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[11], line 44\u001B[0m, in \u001B[0;36mEnhancedLSTM.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     41\u001B[0m c0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# LSTM输出\u001B[39;00m\n\u001B[1;32m---> 44\u001B[0m lstm_out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mh0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc0\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# 注意力权重\u001B[39;00m\n\u001B[0;32m     47\u001B[0m attn_weights \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention(lstm_out), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m   1120\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m   1122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1123\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1124\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1126\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1127\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1128\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1129\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1130\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1132\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1133\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1135\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   1137\u001B[0m         batch_sizes,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1144\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional,\n\u001B[0;32m   1145\u001B[0m     )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6de3caa05a0de851"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
